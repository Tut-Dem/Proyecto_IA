# -*- coding: utf-8 -*-
"""comoEntrenar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11pzS0rs_wX0BA1A1103bBQGOSjg-vLSh
"""

import pandas as pd
import re
import numpy as np
import nltk
from nltk.corpus import stopwords
import seaborn as sns
from tqdm import tqdm
from numpy import array, zeros, asarray
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt


from keras.preprocessing.text import Tokenizer, one_hot
from keras.models import Sequential
from keras.layers import Flatten, Dense, LSTM, Embedding, Dropout,Activation
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

import pickle
with open('df_listo.pkl', 'rb') as f:
    df = pickle.load(f)

with  open('token_fin.pck', 'rb') as f:
  token = pickle.load(f)

with open('matriz_embedding_fin.pkl', 'rb') as f:
  matriz_embedding = pickle.load(f)

#################################
#NO REPLICAR

token = Tokenizer()
token.fit_on_texts(list(df['text']))

################################

vocab = len(token.word_index) + 1
vocab

df['text'] = token.texts_to_sequences(df['text'])

X = list(df['text'])

longitud_maxima= 50
X = pad_sequences(X, maxlen=longitud_maxima, padding='post')

X.shape

X = np.array(X)
y= np.array(df['class'])

######################################################################
# NO REPLICAR

diccionario = dict() # creamos diccionario
with open('glove.6B.100d.txt',encoding='utf-8')as gloVe: # utf v. unidode
  for line in tqdm(gloVe):
    valor = line.split()
    word = valor[0]
    vector= np.asarray(valor[1:],'float32')
    diccionario[word] = vector

matriz_embedding = zeros((vocab,100))
for palabra,indice in token.word_index.items(): # tomamos el indice y la palabra correspondiente del token
  vector_embedding = diccionario.get(palabra)
  if vector_embedding is not None:
    matriz_embedding[indice] = vector_embedding

with open('matriz_embedding_fin.pkl', 'wb') as handle:
  pickle.dump(matriz_embedding,handle )

import pickle
with open('token_fin.pck', 'wb') as handle:
    pickle.dump(token,handle )

#####################################################################################

def crear_modelo(X,y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1200, test_size = 0.3)
  red_neuronal= Sequential()
  red_neuronal.add(Embedding(vocab,100,weights=[matriz_embedding],input_length=longitud_maxima,trainable=False))
  red_neuronal.add(LSTM(60))
  red_neuronal.add(Dense(28,activation='linear'))
  red_neuronal.add(Dropout(0.5))
  red_neuronal.add(Dense(10,activation='relu'))
  red_neuronal.add(Dropout(0.5))
  red_neuronal.add(Dense(5,activation='relu'))
  red_neuronal.add(Dense(1,activation='sigmoid'))

  red_neuronal.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
  red_neuronal.fit(X_train,y_train,epochs=8,batch_size=200,validation_data=(X_test, y_test))

  loss = pd.DataFrame(red_neuronal.history.history)
  plt.figure(figsize=(15,15))
  sns.lineplot(data= loss, lw=3)
  loss.plot()
  return red_neuronal, X_train, X_test, y_train, y_test

red_neu, X_trai, x_test, y_train, y_test=crear_modelo(X,y)

y_pred_test = red_neu.predict(x_test)
y_pred_test = y_pred_test > 0.5
print(f"\n")
print(classification_report(y_test, y_pred_test))
test_matrix = confusion_matrix(y_test, y_pred_test)
print(test_matrix)

red_neu.save('depresionAM.h5')

